{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1: Not a PyTorch Tutorial\n",
    "\n",
    "To work with neural networks we will use PyTorch. We will give a very brief review of the main elements that were are going to use. The objective is not to give a comprehensive PyTorch tutorial. If you want to learn more about PyTorch, you can start with the *official* [tutorials](https://pytorch.org/tutorials/) and [documentation](https://pytorch.org/docs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook can also run on colab (https://colab.research.google.com/)\n",
    "# The following lines install the necessary packages in the colab environment\n",
    "try: \n",
    "    from google.colab import files\n",
    "    !pip install torch==0.4.0\n",
    "    !pip install torchvision \n",
    "    !pip install Pillow==4.0.0\n",
    "    !pip install scikit-image\n",
    "    !pip install hdf5storage\n",
    "    \n",
    "    !pip install git+https://github.com/szagoruyko/pytorchviz\n",
    "    !apt-get install graphviz\n",
    "    \n",
    "    !rm -fr MVA2018-denoising\n",
    "    !git clone  https://github.com/gfacciol/MVA2018-denoising.git\n",
    "    !cp -r MVA2018-denoising/* .\n",
    "\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "\n",
    "## Setup code for the notebook\n",
    "\n",
    "# These are all the includes used through the notebook\n",
    "import numpy as np     \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io # read and write images\n",
    "import vistools        # image visualization toolbox\n",
    "\n",
    "#%matplotlib notebook\n",
    "# Autoreload external python modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data in torch: N dimensional tensors\n",
    "\n",
    "torch stores data in tensors (n-dimensional arrays). Let's create some tensors and show some manipulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import pytorch (or torch)\n",
    "import torch\n",
    "\n",
    "# 5x3 matrix with random entries\n",
    "x = torch.zeros(5,3)\n",
    "print('\\nx = torch.zeros(5,3)')\n",
    "print(x)\n",
    "\n",
    "# a 3D tensor with zeros (3 'channels', 2 rows, 4 columns)\n",
    "x = torch.rand(3,2,4)\n",
    "print('\\nx = torch.rand(3,2,4)')\n",
    "print(x)\n",
    "\n",
    "# check size\n",
    "print('\\nx.size()')\n",
    "print(x.size())\n",
    "\n",
    "# we can add, multiply tensors, etc\n",
    "print('\\nz = 2*torch.ones(x.size())')\n",
    "z = 2*torch.ones(x.size())\n",
    "\n",
    "print('\\nz + x')\n",
    "print(z + x)\n",
    "\n",
    "print('\\nz * x')\n",
    "print(z * x) # element-wise product\n",
    "\n",
    "# we can 'slice' tensors\n",
    "print('\\nx[1,:,:]')\n",
    "print(x[1,:,:]) # second 'channel' of x\n",
    "\n",
    "print('\\nx[:,:,2]')\n",
    "print(x[:,:,2]) # 3rd column of x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions and gradients\n",
    "\n",
    "torch uses backpropagation to compute the gradients of any function of one or more tensors. We will define a simple function fun of two tensors, and compute the gradient with respect to each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we tell torch that we intend to compute a gradient with \n",
    "# respect to these tensors\n",
    "w = torch.rand(1,3, requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "# suppose we aren't interested in the grad of fun w.r.t. x \n",
    "x = torch.rand(1,3)\n",
    "\n",
    "# let's show their values\n",
    "print('\\nw = ')\n",
    "print(w)\n",
    "\n",
    "print('\\nx = ')\n",
    "print(x)\n",
    "\n",
    "print('\\nb = ')\n",
    "print(b)\n",
    "\n",
    "# now let's define fun as fun(w,x,b) = <w, x> + b\n",
    "fun = (w * x).sum() + b\n",
    "\n",
    "# For torch, fun is a tensor, but it has kept track of the \n",
    "# operations performed so far, and is thus able to run \n",
    "# the back propagation algorithm to compute the gradients\n",
    "print('\\nfun = (w * x).sum() + b =')\n",
    "print(fun)\n",
    "\n",
    "# to run backprop, just call\n",
    "fun.backward()\n",
    "\n",
    "# The gradients of fun w.r.t. all parameters having \n",
    "# requires_grad=True have been computed. We can access them\n",
    "\n",
    "print('\\ndfun/dw =')\n",
    "print(w.grad) # grad of fun w.r.t. w\n",
    "\n",
    "print('\\ndfun/db =')\n",
    "print(b.grad) # grad of fun w.r.t. b\n",
    "\n",
    "print('\\ndfun/dx =')\n",
    "print(x.grad) # grad of fun w.r.t. x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our first network\n",
    "\n",
    "We have all elements to define a neural network and train it: we can define functions of tensors and compute any of its gradients. In the following example, we will define an affine fc layer in a neural net as a class with parameters weights and biases.\n",
    "\n",
    "**Note:** A *class* is just a way of bounding together data and functions that use these data. In this case the data are going to be the weights, and the function is the actual computation of the fc layer given an input x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_layer():\n",
    "    \"\"\" \n",
    "    An example fc_layer class.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The __init__ functions initializes the class when it's created\n",
    "    # Every class has an __init__ funcion. Don't pay attention to the\n",
    "    # self parameter.\n",
    "    def __init__(self, in_size, out_size):\n",
    "        \n",
    "        # initialize random weights distributed as N(0,1) and 0 bias\n",
    "        self.weight = torch.randn(out_size, in_size, requires_grad=True)\n",
    "        self.bias = torch.zeros(out_size, 1, requires_grad=True)\n",
    "        \n",
    "    # We define a forward function that applies the fc layer\n",
    "    # to a tensor x\n",
    "    def forward(self, x):\n",
    "        # torch.mm(A,B) is the matrix multiplication AB\n",
    "        return torch.mm(self.weight, x) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are done with the definition of our fc class. Let's use it to \n",
    "# a network with two layers (a hidden layer with a tanh non-linearity\n",
    "# and an output).\n",
    "\n",
    "fc1 = fc_layer(3, 5) # first fc layer\n",
    "fc2 = fc_layer(5, 4) # output fc layer\n",
    "\n",
    "# Let's look at a layer\n",
    "print('Initial parameters of fc layer 1:')\n",
    "print(fc1.weight)\n",
    "print(fc1.bias)\n",
    "\n",
    "# Let's test it on some input data\n",
    "x = torch.randn(3,1)\n",
    "\n",
    "# Run forward pass of the network\n",
    "out = fc1.forward(x)   # first fc layer\n",
    "out = torch.tanh(out)  # activation function\n",
    "out = fc2.forward(out) # output fc layer\n",
    "\n",
    "print('\\nNetworks output:')\n",
    "print(out)\n",
    "\n",
    "\n",
    "# During training, we know the desired output or 'label' y for x, and\n",
    "# want to minimize the loss. \n",
    "\n",
    "y = torch.randn(4,1) # we invent a desired output\n",
    "\n",
    "# Let's use the squared L2 norm as loss\n",
    "loss = ((out - y)**2).sum()\n",
    "\n",
    "print('\\nLoss between networks output and label:')\n",
    "print(loss)\n",
    "\n",
    "# To compute the gradients, we backprop from the loss\n",
    "loss.backward()\n",
    "\n",
    "# gradients with respect to the first layer's params\n",
    "print('\\nGradient of the loss w.r.t. first layer params:')\n",
    "print(fc1.weight.grad)\n",
    "print(fc1.bias.grad)\n",
    "\n",
    "# gradients with respect to the first layer's params\n",
    "print('\\nGradient of the loss w.r.t. second layer params:')\n",
    "print(fc2.weight.grad)\n",
    "print(fc2.bias.grad)\n",
    "\n",
    "# Note how the gradients w.r.t. the first layer's weight are much\n",
    "# smaller, as a consequence of the tanh saturation\n",
    "\n",
    "# To train, we loop over a data set extracting mini-batches of data.\n",
    "# We compute the loss over each mini-batch (i.e. he loss is the sum \n",
    "# over the mini-batch, instead of a single x), and compute the gradients\n",
    "# using backpropagation.\n",
    "# The parameters are updated according to an update rule. In case of the \n",
    "# gradient descent, the rule is simply:\n",
    "learning_rate = 1e-3\n",
    "fc1.weight = fc1.weight - learning_rate * fc1.weight.grad\n",
    "fc1.bias   = fc1.bias   - learning_rate * fc1.bias.grad\n",
    "fc2.weight = fc2.weight - learning_rate * fc2.weight.grad\n",
    "fc2.bias   = fc2.bias   - learning_rate * fc2.bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A CNN using torch modules\n",
    "\n",
    "torch has a series of modules to facilitate defining and training neural networks. The torch.nn has a large number of useful classes implementing layers and combination of layers. The previous fc net could be implemented using `torch.nn` layers as follows:\n",
    "\n",
    "`fc1 = torch.nn.Linear(3,5)`<br>\n",
    "`fc2 = torch.nn.Linear(5,4)`\n",
    "\n",
    "We will define our second network: a convolutional network with 2 layers and ReLU activations. In practice, it's useful (if not necessary) to encapsulate the whole network in a class. Let's show an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_cnn(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A network with two conv layers and a ReLU non-linearity.\n",
    "\n",
    "    Note: the class inherits from torch.nn.Module. Don't worry if you are not\n",
    "    familiar with classes and inheritance, you can ignore this comment. Essentially,\n",
    "    it means that the functions and data of torch.nn.Module are automatically\n",
    "    available here.\n",
    "\n",
    "    It has the following parameters:\n",
    "        - im_channels: number of input and output image channels\n",
    "        - num_features: number of features (or channels) of the hidden layer\n",
    "        - kernel_size: size of convolution kernels on both layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, im_channels, num_features, kernel_size):\n",
    "        super(simple_cnn, self).__init__() # related with the inheritance from nn.Module\n",
    "\n",
    "        # create convolutional layers (parameters are initialized at random)\n",
    "        self.conv1 = torch.nn.Conv2d(im_channels, num_features, kernel_size)\n",
    "        self.conv2 = torch.nn.Conv2d(num_features, im_channels, kernel_size)\n",
    "\n",
    "        # relu activation ('inplace' to overwrite input data with output)\n",
    "        self.relu  = torch.nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # run the network\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# We can now create a network. For example one image channel, 3 hidden features, 5x5 kernels\n",
    "net = simple_cnn(1,3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print its parameters (check the sizes of the weights)\n",
    "print('\\n1st layer convolution kernels:')\n",
    "print(net.conv1.weight.size())\n",
    "print(net.conv1.weight)\n",
    "print('\\n1st layer biases:')\n",
    "print(net.conv1.bias)\n",
    "print('\\n2nd layer convolution kernels:')\n",
    "print(net.conv2.weight.size())\n",
    "print(net.conv2.weight)\n",
    "print('\\n2nd layer biases:')\n",
    "print(net.conv2.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will apply our network to an image.\n",
    "# Try running this several times and changing parameters.\n",
    "net = simple_cnn(1,5,3)\n",
    "\n",
    "# Load an image to test the net on it\n",
    "image = io.imread('datasets/BSD68/test002.png', dtype='float32')\n",
    "\n",
    "# Let's convert it to a 4D torch floating point tensor\n",
    "# The input data to convolutional layers in torch has to be 4D\n",
    "# since it expects a mini-batch, the size is:\n",
    "# mini-batch_size x channels x height x with\n",
    "img = torch.FloatTensor(image[np.newaxis, np.newaxis, :, :])\n",
    "\n",
    "# Now we can test our network in on the image. \n",
    "# The following line tells torch that we don't want to propagate.\n",
    "with torch.no_grad():\n",
    "    # covert it as a 4D torch tensor\n",
    "    out = net.forward(img)\n",
    "\n",
    "# Let's scale the output's range to 0, 255.\n",
    "out_scaled = (out - out.min())*255./(out.max() - out.min())\n",
    "\n",
    "# Show as a gallery\n",
    "vistools.display_gallery([out_scaled.numpy().clip(0,255), image], \n",
    "                         ['scaled output', 'input'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's left?\n",
    "\n",
    "torch we covered tensors, automatic differentiation using backprop and construction of neural nets using the `torch.nn` packages.\n",
    "\n",
    "torch has other very useful packages, but we won't cover them in this tutorial. We will use two of them in the rest of the TP:\n",
    "- `torch.optim` is a package implementing various optimization algorithms. Most commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can be also easily integrated in the future. This simplifies a lot the update of the weights.\n",
    "- `torch.utils.data` a set of tools for handling datasets, such as splitting a data set in training, evaluation and testing, loading mini-batches, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
